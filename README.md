We propose Drive-In Guard, a hierarchical, risk-aware benchmark and guardrail framework designed to evaluate and mitigate unsafe behavior in large language models (LLMs) deployed in driving assistant chatbot scenarios. While existing efforts have introduced general-purpose risk taxonomies for LLM safety, they lack the domain-specific detail required for high-risk environments like in-vehicle conversations. To address these challenges, our framework introduces a manually curated, multi-level risk taxonomy tailored specifically to driving-related interactions, and a custom LangChain-based Retrieval-Augmented Generation (RAG) guardrail that improves refusal behavior in unsafe situations. Evaluated across six commercial LLMs, Drive-In Guard enables structured risk labeling and significantly improves refusal precision and safety alignment. To the best of our knowledge, this is the first domain-specific benchmark and safeguard framework targeting LLM safety in driving-oriented chatbot applications.
